{"cells":[{"cell_type":"markdown","source":["# This notebook covers Reinforcement learning algorithm i.e. Q Learning."],"metadata":{"id":"MqTfEgnFqAW9"},"id":"MqTfEgnFqAW9"},{"cell_type":"code","execution_count":1,"id":"b51830da-0d4b-4271-9566-3459ebf64c04","metadata":{"id":"b51830da-0d4b-4271-9566-3459ebf64c04","executionInfo":{"status":"ok","timestamp":1758556607977,"user_tz":-300,"elapsed":62,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"outputs":[],"source":["# Import NumPy and Random libraries.\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":2,"id":"a9542d16-9086-499a-a5b6-931deb8a57f0","metadata":{"id":"a9542d16-9086-499a-a5b6-931deb8a57f0","executionInfo":{"status":"ok","timestamp":1758556609387,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"498f2883-8d1b-49df-9b55-b0f603c258d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Grid Environment:\n","[['S' 'F' 'F' 'F']\n"," ['F' 'H' 'F' 'H']\n"," ['F' 'F' 'F' 'H']\n"," ['H' 'F' 'F' 'G']]\n","\n"]}],"source":["# Define the gridworld environment.\n","# 'S' represents the starting point, 'G' represents the goal, 'H' represents\n","# a hole, and 'F' represents a regular empty cell.\n","# The agent can move in four directions: up, down, left, and right.\n","\n","grid = np.array([\n","    ['S', 'F', 'F', 'F'],\n","    ['F', 'H', 'F', 'H'],\n","    ['F', 'F', 'F', 'H'],\n","    ['H', 'F', 'F', 'G']\n","])\n","\n","print(\"Grid Environment:\")\n","print(grid)\n","print()"]},{"cell_type":"code","execution_count":3,"id":"3ae27441-b245-42c6-9050-4b854594f49c","metadata":{"id":"3ae27441-b245-42c6-9050-4b854594f49c","executionInfo":{"status":"ok","timestamp":1758556618485,"user_tz":-300,"elapsed":6,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"outputs":[],"source":["# Define the rewards for each state.\n","rewards = {\n","    'G': 100, # Goal state\n","    'H': -100, # Hole state\n","    'S': 0, # Start state\n","    'F': -1 # Empty cell\n","}"]},{"cell_type":"code","execution_count":4,"id":"8c723d0f-51ec-4136-a7e6-b6a854353d9f","metadata":{"id":"8c723d0f-51ec-4136-a7e6-b6a854353d9f","executionInfo":{"status":"ok","timestamp":1758556621008,"user_tz":-300,"elapsed":18,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"outputs":[],"source":["# Define parameters.\n","learning_rate = 0.1\n","discount_factor = 0.9\n","num_episodes = 1000"]},{"cell_type":"code","execution_count":5,"id":"b3d6c72a-b24a-486c-bb7f-8f112ce41d1d","metadata":{"id":"b3d6c72a-b24a-486c-bb7f-8f112ce41d1d","executionInfo":{"status":"ok","timestamp":1758556646882,"user_tz":-300,"elapsed":8,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"outputs":[],"source":["# Initialize the Q-table with zeros\n","num_states = np.prod(np.shape(grid))\n","num_actions = 4 # Up, Down, Left, Right\n","Q = np.zeros((num_states, num_actions))"]},{"cell_type":"code","source":["# Helper function to convert (row, col) to state index\n","def state_index(row, col):\n","    return row * len(grid[0]) + col\n","\n","# Helper function to convert state index to (row, col)\n","def index_to_state(index):\n","    row = index // len(grid[0])\n","    col = index % len(grid[0])\n","    return row, col\n","\n","# Epsilon-greedy action selection\n","def choose_action(state, epsilon=0.1):\n","    if random.random() < epsilon:\n","        return random.randint(0, num_actions - 1)  # Explore: random action\n","    else:\n","        return np.argmax(Q[state, :])  # Exploit: best action\n"],"metadata":{"id":"wPuFMK5bJON6","executionInfo":{"status":"ok","timestamp":1758556676407,"user_tz":-300,"elapsed":7,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"id":"wPuFMK5bJON6","execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"id":"b16ec879-bee1-474d-ad7b-3d41d443a39e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b16ec879-bee1-474d-ad7b-3d41d443a39e","executionInfo":{"status":"ok","timestamp":1758556737231,"user_tz":-300,"elapsed":1335,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}},"outputId":"702ad055-534d-4501-996a-ae3525752e3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 200: Average reward (last 100): -17.19\n","Episode 400: Average reward (last 100): -16.43\n","Episode 600: Average reward (last 100): -13.91\n","Episode 800: Average reward (last 100): 15.19\n","Episode 1000: Average reward (last 100): 53.06\n","\n","Training completed!\n","Final average reward (last 100 episodes): 53.06\n","\n","Q-table:\n","[[ 16.73064776  54.95380349  11.86613894   0.52595009]\n"," [ -0.99427358 -98.92247363   0.           7.36712881]\n"," [ -0.88976544  25.41907768  -0.92023356  -1.02309064]\n"," [ -0.95617925 -10.          -0.99792227  -0.95617925]\n"," [ 17.30648918  62.17097688  21.71908456 -99.75349653]\n"," [  0.           0.           0.           0.        ]\n"," [ -0.45816856  62.18239809 -27.1        -19.        ]\n"," [  0.           0.           0.           0.        ]\n"," [ 18.00130586 -68.61894039  28.28293263  70.18999384]\n"," [-61.2579511   16.12807992  37.50001803  79.09999846]\n"," [ 17.94466193  88.99999985  36.43930569 -61.2579511 ]\n"," [  0.           0.           0.           0.        ]\n"," [  0.           0.           0.           0.        ]\n"," [ -0.216658    -0.199      -19.          58.01909317]\n"," [ 43.44976367  50.59226403   7.06838252  99.99999999]\n"," [  0.           0.           0.           0.        ]]\n"]}],"source":["# Q-learning algorithm.\n","episode_rewards = []\n","\n","for episode in range(num_episodes):\n","    state = state_index(0, 0)  # Start from the top-left corner (S)\n","    done = False\n","    total_reward = 0\n","    steps = 0\n","    max_steps = 100  # Prevent infinite loops\n","\n","    while not done and steps < max_steps:\n","        action = choose_action(state, epsilon=0.1)  # Using epsilon-greedy policy\n","        current_row, current_col = index_to_state(state)\n","\n","        # Calculate next position based on action\n","        next_row, next_col = current_row, current_col\n","\n","        if action == 0:  # Up\n","            next_row = max(0, current_row - 1)\n","        elif action == 1:  # Down\n","            next_row = min(len(grid) - 1, current_row + 1)\n","        elif action == 2:  # Left\n","            next_col = max(0, current_col - 1)\n","        elif action == 3:  # Right\n","            next_col = min(len(grid[0]) - 1, current_col + 1)\n","\n","        next_state = state_index(next_row, next_col)\n","        reward = rewards[grid[next_row, next_col]]\n","        total_reward += reward\n","\n","        # Q-learning update rule\n","        Q[state, action] += learning_rate * (\n","            reward + discount_factor * np.max(Q[next_state, :]) - Q[state, action]\n","        )\n","\n","        state = next_state\n","        steps += 1\n","\n","        # Check if episode is done (reached goal or hole)\n","        if grid[next_row, next_col] == 'G' or grid[next_row, next_col] == 'H':\n","            done = True\n","\n","    episode_rewards.append(total_reward)\n","\n","    # Print progress every 100 episodes\n","    if (episode + 1) % 200 == 0:\n","        avg_reward = np.mean(episode_rewards[-100:])\n","        print(f\"Episode {episode + 1}: Average reward (last 100): {avg_reward:.2f}\")\n","\n","print(\"\\nTraining completed!\")\n","print(f\"Final average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n","\n","print(\"\\nQ-table:\")\n","print(Q)\n"]},{"cell_type":"code","execution_count":8,"id":"c19b6a6d-8edc-409d-8e1f-f886761ded11","metadata":{"id":"c19b6a6d-8edc-409d-8e1f-f886761ded11","outputId":"7e39026d-50f4-4875-a804-0a6e83ff858b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758556804523,"user_tz":-300,"elapsed":15,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Optimal Policy:\n","[['Down' 'Right' 'Down' 'Up']\n"," ['Down' 'Hole' 'Down' 'Hole']\n"," ['Right' 'Right' 'Down' 'Hole']\n"," ['Hole' 'Right' 'Right' 'Up']]\n"]}],"source":["# Use the learned Q-table to find the optimal policy.\n","optimal_policy = []\n","action_names = ['Up', 'Down', 'Left', 'Right']\n","\n","print(\"\\nOptimal Policy:\")\n","for row in range(len(grid)):\n","    policy_row = []\n","    for col in range(len(grid[0])):\n","        if grid[row, col] == 'H':\n","            policy_row.append('Hole')\n","        else:\n","            state = state_index(row, col)\n","            best_action = np.argmax(Q[state, :])\n","            policy_row.append(action_names[best_action])\n","    optimal_policy.append(policy_row)\n","\n","optimal_policy = np.array(optimal_policy)\n","print(optimal_policy)"]},{"cell_type":"code","source":["# Test the learned policy\n","def test_policy(start_row=0, start_col=0, max_steps=20):\n","    print(f\"\\nTesting policy starting from ({start_row}, {start_col}):\")\n","    row, col = start_row, start_col\n","    path = [(row, col)]\n","\n","    for step in range(max_steps):\n","        print(f\"Step {step + 1}: Position ({row}, {col}) - Cell: {grid[row, col]}\")\n","\n","        if grid[row, col] == 'G':\n","            print(\"Reached Goal!\")\n","            break\n","        elif grid[row, col] == 'H':\n","            print(\"Fell into Hole!\")\n","            break\n","\n","        state = state_index(row, col)\n","        action = np.argmax(Q[state, :])\n","\n","        # Move based on action\n","        if action == 0:  # Up\n","            row = max(0, row - 1)\n","        elif action == 1:  # Down\n","            row = min(len(grid) - 1, row + 1)\n","        elif action == 2:  # Left\n","            col = max(0, col - 1)\n","        elif action == 3:  # Right\n","            col = min(len(grid[0]) - 1, col + 1)\n","\n","        path.append((row, col))\n","        print(f\"Action: {action_names[action]} -> New position: ({row}, {col})\")\n","\n","    return path\n","\n","# Test the learned policy\n","test_path = test_policy()\n","print(f\"\\nPath taken: {test_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SiCmvKJtIloh","executionInfo":{"status":"ok","timestamp":1758556819315,"user_tz":-300,"elapsed":37,"user":{"displayName":"Sumayyea Salahuddin Lectr DCSE Dept","userId":"12039789708672274917"}},"outputId":"8c705959-b295-44bd-ba15-958acf7a3113"},"id":"SiCmvKJtIloh","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing policy starting from (0, 0):\n","Step 1: Position (0, 0) - Cell: S\n","Action: Down -> New position: (1, 0)\n","Step 2: Position (1, 0) - Cell: F\n","Action: Down -> New position: (2, 0)\n","Step 3: Position (2, 0) - Cell: F\n","Action: Right -> New position: (2, 1)\n","Step 4: Position (2, 1) - Cell: F\n","Action: Right -> New position: (2, 2)\n","Step 5: Position (2, 2) - Cell: F\n","Action: Down -> New position: (3, 2)\n","Step 6: Position (3, 2) - Cell: F\n","Action: Right -> New position: (3, 3)\n","Step 7: Position (3, 3) - Cell: G\n","Reached Goal!\n","\n","Path taken: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (3, 2), (3, 3)]\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}